import os
os.environ['GRPC_VERBOSITY'] = 'NONE'

# Qdrant for RAG vector DB
from langchain_qdrant import QdrantVectorStore
from qdrant_client import QdrantClient
from qdrant_client.http.models import Distance, VectorParams
from langchain_openai import OpenAIEmbeddings

from langchain_qdrant import FastEmbedSparse
from langchain_qdrant import QdrantVectorStore
from langchain_google_genai import GoogleGenerativeAIEmbeddings
from langchain_core.runnables import RunnablePassthrough
from langchain_core.output_parsers import StrOutputParser
from langchain_core.prompts import ChatPromptTemplate
from langchain_google_genai import ChatGoogleGenerativeAI
from setup_env import setup_env
from langchain_community.document_loaders import PyPDFLoader

from langchain_core.documents import Document
from uuid import uuid4

def load_docs(doc_path):
    pdf_loader = PyPDFLoader(doc_path)
    docs = pdf_loader.load()
    ids = [str(uuid4()) for _ in range(len(docs))]
    return docs, ids

def build_vector_store():
    embedding_model_name = "models/text-embedding-004"
    embedding_model = GoogleGenerativeAIEmbeddings(model=embedding_model_name)
    client = QdrantClient(":memory:")

    client.create_collection(
        collection_name="research_paper",
        vectors_config=VectorParams(size=768, distance=Distance.COSINE),
    )

    vector_store = QdrantVectorStore(
        client=client,
        collection_name="research_paper",
        embedding=embedding_model,
    )
    return vector_store


def get_generator_chain(vector_store):
    creator_prompt = """
    Persona: You are a research expert and can help with reading through and summarizing research papers and answering questions.
    Action: You will take the user question and look for its answer in the provided "Document Context" and respond. If there is 
    a previous response and/or improvement suggestion provided, you will use the suggestions to improve upon the previous response.
    Restrictions: If found, respond with the answer. If you cannot find the answer in the text, respond with "Not a relevant question".
    Document Context:
    {context}

    User Question:
    {question}

    Previous Response:
    {response}

    Improvement Suggestions:
    {improvements}
    """
    # Define the prompt template
    prompt = ChatPromptTemplate.from_template(creator_prompt)
    # Initialize the Chat Model
    # Make sure to replace "models/gemini-1.5-pro-latest" with a model from the list you generated if needed
    llm = ChatGoogleGenerativeAI(model="models/gemini-2.5-pro-preview-03-25")

    # Create a retriever with similarity score threshold
    retriever = vector_store.as_retriever(
        search_type="similarity_score_threshold",
        search_kwargs={'score_threshold': 0.5, 'k': 10} # Adjusted threshold and number of chunks
    )

    # Create the RAG chain using the pipe operator
    rag_chain = (
        {"context": retriever, "question": RunnablePassthrough()}
        | prompt
        | llm
        | StrOutputParser()
    )

    print("Generator RAG chain created.")
    return rag_chain

def get_evaluator_chain(vector_store):
    evaluator_prompt = """
    Persona: You are a research expert to evaluate other researcher's response to a question based on a document context.
    Action: You will take the context, user question and the answer generated by another researcher and evaluate if the answer can be improved.
    Restrictions: If the answer can be improved, you will output - "Needs improvement. Here is how you can improve..." and you will explain 
    through a few points how to improve the response. If the answer looks good to you, simply output "Great response."
    Document Context:
    {context}

    User Question:
    {question}

    Researcher Response:
    {response}
    """
    # Define the prompt template
    prompt = ChatPromptTemplate.from_template(evaluator_prompt)
    # Initialize the Chat Model
    # Make sure to replace "models/gemini-1.5-pro-latest" with a model from the list you generated if needed
    llm = ChatGoogleGenerativeAI(model="models/gemini-2.5-pro-preview-03-25")

    # Create a retriever with similarity score threshold
    retriever = vector_store.as_retriever(
        search_type="similarity_score_threshold",
        search_kwargs={'score_threshold': 0.5, 'k': 10} # Adjusted threshold and number of chunks
    )

    # Create the RAG chain using the pipe operator
    rag_chain = (
        {"context": retriever, "question": RunnablePassthrough()}
        | prompt
        | llm
        | StrOutputParser()
    )

    print("Generator RAG chain created.")
    return rag_chain

def main():
    setup_env()
    documents, ids = load_docs("content/fnut-09-960922.pdf")
    vector_store = build_vector_store()
    vector_store.add_documents(documents=documents, ids=ids)

    

    

    gen_chain = get_rag_chain(creator_prompt, vector_store)
    eval_chain = get_rag_chain(evaluator_prompt, vector_store)
    output = lc.invoke("Give me a 10 sentence summary on fad diets")
    print(output)

if __name__ == "__main__":
    main()