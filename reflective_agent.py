import os
from typing import List, TypedDict

os.environ['GRPC_VERBOSITY'] = 'NONE'

# Qdrant for RAG vector DB
from langchain_qdrant import QdrantVectorStore
from qdrant_client import QdrantClient
from qdrant_client.http.models import Distance, VectorParams

from langchain_google_genai import GoogleGenerativeAIEmbeddings
from langchain_core.runnables import RunnablePassthrough
from langchain_core.output_parsers import StrOutputParser
from langchain_core.prompts import ChatPromptTemplate
from langchain_google_genai import ChatGoogleGenerativeAI
from setup_env import setup_env
from langchain_community.document_loaders import PyPDFLoader

from langchain_core.documents import Document
from uuid import uuid4

from langgraph.graph import END, StateGraph
from pprint import pprint


class GraphState(TypedDict):
    """
    Represents the state of our graph.

    Attributes:
        question: question
        generation: LLM generation
        documents: list of documents
        improvements: any improvement suggestions
        iteration: iteration number
    """
    question: str
    generation: str
    documents: List[str]
    improvements: str
    iteration: int
    

class GraphContext(TypedDict):
    """
    Represents the context of our graph.

    Attributes:
        generate_model: model used in generate node
        evaluate_model: model used in evaluate node
    """
    generate_model: str
    evaluate_model: str

def load_docs(doc_path):
    pdf_loader = PyPDFLoader(doc_path)
    docs = pdf_loader.load()
    ids = [str(uuid4()) for _ in range(len(docs))]
    return docs, ids

def build_vector_store():
    embedding_model_name = "models/text-embedding-004"
    embedding_model = GoogleGenerativeAIEmbeddings(model=embedding_model_name)
    client = QdrantClient(":memory:")

    client.create_collection(
        collection_name="research_paper",
        vectors_config=VectorParams(size=768, distance=Distance.COSINE),
    )

    vector_store = QdrantVectorStore(
        client=client,
        collection_name="research_paper",
        embedding=embedding_model,
    )
    return vector_store

def retrieve(state, runtime):
    """
    Retrieve documents

    Args:
        state (dict): The current graph state

    Returns:
        state (dict): New key added to state, documents, that contains retrieved documents
    """
    print("---RETRIEVING DOCUMENTS---")
    question = state["question"]
    documents = retriever.get_relevant_documents(question)
    return {"documents": documents, "question": question}


def generate(state, runtime):
    """
    Generate answer

    Args:
        state (dict): The current graph state

    Returns:
        state (dict): New key added to state, generation, that contains LLM generation
    """
    print("---GENERATE---")
    question = state["question"]
    documents = state["documents"]
    iteration = state.get("iteration", 0)
    improvements = state.get("improvements")


    creator_prompt = """
    Persona: You are a research expert and can help with reading through and summarizing research papers and answering questions.
    Action: You will take the user question and look for its answer in the provided "Document Context" and respond. If there is
    a previous response and/or improvement suggestion provided, you will use the suggestions to improve upon the previous response.
    Restrictions: If found, respond with the answer. If you cannot find the answer in the text, respond with "Not a relevant question".
    Document Context:
    {context}

    User Question:
    {question}

    Previous Response:
    {response}

    Improvement Suggestions:
    {improvements}
    """
    # Define the prompt template
    prompt = ChatPromptTemplate.from_template(creator_prompt)
    # Initialize the Chat Model
    llm = ChatGoogleGenerativeAI(model=runtime.context.get("evaluate_model"))


    # Create the RAG chain using the pipe operator
    rag_chain = (
        prompt
        | llm
        | StrOutputParser()
    )
    generation = rag_chain.invoke({"context": documents, "question": question, "response": "", "improvements": improvements})
    return {"documents": documents, "question": question, "generation": generation, "iteration": iteration + 1}


def evaluate(state, runtime):
    """
    Determines whether the generation is grounded in the document and answers the question.

    Args:
        state (dict): The current graph state

    Returns:
        state (dict): Updates documents key with relevant documents
    """
    print("---CHECK DOCUMENT GROUNDEDNESS---")
    question = state["question"]
    documents = state["documents"]
    generation = state["generation"]
    iteration = state["iteration"]


    evaluator_prompt = """
    Persona: You are a research expert to evaluate other researcher's response to a question based on a document context.
    Action: You will take the context, user question and the answer generated by another researcher and evaluate if the answer can be improved.
    Restrictions: If the answer can be improved, you will output - "Needs improvement. Here is how you can improve..." and you will explain
    through a few points how to improve the response. If the answer looks good to you, simply output "Great response."
    Document Context:
    {context}

    User Question:
    {question}

    Researcher Response:
    {response}
    """
    prompt = ChatPromptTemplate.from_template(evaluator_prompt)
    llm = ChatGoogleGenerativeAI(model=runtime.context.get("evaluate_model"))
    
    rag_chain = (
        prompt
        | llm
        | StrOutputParser()
    )

    improvements = rag_chain.invoke({"context": documents, "question": question, "response": generation})
    return {"generation": generation, "improvements": improvements, "iteration": iteration}


def decide_to_finish(state, runtime):
    """
    Determines whether to finish the process or continue with another iteration.

    Args:
        state (dict): The current graph state

    Returns:
        str: "finish" if the generation is good, "continue" otherwise
    """
    print("---DECIDE TO FINISH---")
    iteration = state["iteration"]
    improvements = state["improvements"]

    if "Great response." in improvements or iteration >= 3:
        print("---DECISION: FINISH---")
        return "finish"
    else:
        print("---DECISION: CONTINUE---")
        return "continue"
    

def main():
    setup_env()
    documents, ids = load_docs("content/fnut-09-960922.pdf")
    vector_store = build_vector_store()
    vector_store.add_documents(documents=documents, ids=ids)
    
    global retriever
    retriever = vector_store.as_retriever(
        search_type="similarity_score_threshold",
        search_kwargs={'score_threshold': 0.5, 'k': 10}
    )


    workflow = StateGraph(GraphState)

    # Define the nodes
    workflow.add_node("retrieve", retrieve)
    workflow.add_node("generate", generate)
    workflow.add_node("evaluate", evaluate)

    # Build graph
    workflow.set_entry_point("retrieve")
    workflow.add_edge("retrieve", "generate")
    workflow.add_edge("generate", "evaluate")
    workflow.add_conditional_edges(
        "evaluate",
        decide_to_finish,
        {
            "continue": "generate",
            "finish": END,
        },
    )

    app = workflow.compile()
    
    inputs = {"question": "Give me a 10 sentence summary on fad diets"}

    for output in app.stream(inputs, context={
        "generate_model": "models/gemini-2.5-pro-preview-03-25", 
        "evaluate_model": "models/gemini-2.5-pro-preview-03-25"
        }):
        for okey, ovalue in output.items():
            print(f"Output from node '{okey}':")
            print("---")
            for ikey, ivalue in ovalue.items():
                if ikey == "documents":
                    continue
                pprint(f"{ikey}:{ivalue}")
            print("\n---\n")
        
    pprint(ovalue['generation'])


if __name__ == "__main__":
    main()